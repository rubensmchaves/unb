{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOH+laZRZdkdERfkmrf/St3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Environment settings"],"metadata":{"id":"PNWYIbAvyGGt"}},{"cell_type":"markdown","source":["Download python files from the repository"],"metadata":{"id":"Kxd0THqCtAWt"}},{"cell_type":"code","source":["from importlib.metadata import version\n","\n","try:\n","  import tensorflow\n","  import tqdm\n","except ImportError:\n","  !pip install tensorflow tqdm\n","  import tensorflow\n","  import tqdm\n","\n","print(\"TensorFlow version:\", version(\"tensorflow\"))\n","print(\"tqdm version:\", version(\"tqdm\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"clnUqPPXjiHx","executionInfo":{"status":"ok","timestamp":1739547234381,"user_tz":180,"elapsed":14836,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"346d564a-4921-403a-97c6-80d6f0bbf678"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version: 2.18.0\n","tqdm version: 4.67.1\n"]}]},{"cell_type":"code","source":["try:\n","  import tiktoken\n","except ImportError:\n","  !pip install tiktoken\n","  import tiktoken\n","\n","print(\"TikTokenizer version:\", version(\"tiktoken\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3uTASPfn4mA","executionInfo":{"status":"ok","timestamp":1739547238752,"user_tz":180,"elapsed":4378,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"e04fc1a6-0c48-4f7f-ded8-284821269a06"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken\n","Successfully installed tiktoken-0.9.0\n","TikTokenizer version: 0.9.0\n"]}]},{"cell_type":"code","source":["try:\n","  import torch\n","except ImportError:\n","  !pip install torch\n","  import torch\n","\n","print(\"Torch version:\", version(\"torch\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIcljw4l2wiv","executionInfo":{"status":"ok","timestamp":1739547249026,"user_tz":180,"elapsed":10278,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"c0baaf42-abcc-47d8-c542-5189e82c40e0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Torch version: 2.5.1+cu124\n"]}]},{"cell_type":"code","source":["# Relative import from the gpt_download.py contained in this folder\n","try:\n","  from gpt_download import download_and_load_gpt2\n","except ImportError:\n","  !wget {\"https://raw.githubusercontent.com/rubensmchaves/unb/refs/heads/main/nlp/A05_gpt/gpt_download.py\"}\n","  from gpt_download import download_and_load_gpt2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QZBo46IIivd6","executionInfo":{"status":"ok","timestamp":1739547249288,"user_tz":180,"elapsed":268,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"3662c7fa-6dd1-4a52-9504-21d02de74c24"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-14 15:34:44--  https://raw.githubusercontent.com/rubensmchaves/unb/refs/heads/main/nlp/A05_gpt/gpt_download.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6310 (6.2K) [text/plain]\n","Saving to: ‘gpt_download.py’\n","\n","gpt_download.py     100%[===================>]   6.16K  --.-KB/s    in 0s      \n","\n","2025-02-14 15:34:44 (60.6 MB/s) - ‘gpt_download.py’ saved [6310/6310]\n","\n"]}]},{"cell_type":"code","source":["# Relative import from the gpt.py contained in this folder\n","try:\n","  from gpt import GPTModel\n","except ImportError:\n","  !wget {\"https://raw.githubusercontent.com/rubensmchaves/unb/refs/heads/main/nlp/A05_gpt/gpt.py\"}\n","  from gpt import GPTModel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0AFGvYAsnVoP","executionInfo":{"status":"ok","timestamp":1739547249593,"user_tz":180,"elapsed":309,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"ec30dcb0-a54e-4159-a58d-98eb2a5cceb1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-14 15:34:44--  https://raw.githubusercontent.com/rubensmchaves/unb/refs/heads/main/nlp/A05_gpt/gpt.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14935 (15K) [text/plain]\n","Saving to: ‘gpt.py’\n","\n","gpt.py              100%[===================>]  14.58K  --.-KB/s    in 0.001s  \n","\n","2025-02-14 15:34:44 (14.0 MB/s) - ‘gpt.py’ saved [14935/14935]\n","\n"]}]},{"cell_type":"markdown","source":["# Basic GPT model configuration"],"metadata":{"id":"0b2oABwCrmTR"}},{"cell_type":"markdown","source":["We initialize a basic GPT model configuration"],"metadata":{"id":"hK0_YeLorzmK"}},{"cell_type":"code","source":["GPT_CONFIG_124M = {\n","    \"vocab_size\": 50257,     # Vocabulary size\n","    \"context_length\": 1024,  # Context length\n","    \"emb_dim\": 768,          # Embedding dimension\n","    \"n_heads\": 12,           # Number of attention heads\n","    \"n_layers\": 12,          # Number of layers\n","    \"drop_rate\": 0.1,        # Dropout rate\n","    \"qkv_bias\": False        # Query-Key-Value bias\n","}"],"metadata":{"id":"mx11ZZcRnJn_","executionInfo":{"status":"ok","timestamp":1739547249593,"user_tz":180,"elapsed":6,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Download and load the Large Language Model (LLM)"],"metadata":{"id":"0ouU6GX9lOGQ"}},{"cell_type":"markdown","source":["Download the model weights for the 124 million parameter model as follows:"],"metadata":{"id":"Cm505Tf_qyF4"}},{"cell_type":"code","source":["settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECNs5a3MlXC3","executionInfo":{"status":"ok","timestamp":1739547272077,"user_tz":180,"elapsed":22489,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"003bf8bb-e6d7-4c40-dae9-dac0f45ab631"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 60.4kiB/s]\n","encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.98MiB/s]\n","hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 160kiB/s]\n","model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:20<00:00, 24.8MiB/s]\n","model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 3.90MiB/s]\n","model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 3.35MiB/s]\n","vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.00MiB/s]\n"]}]},{"cell_type":"code","source":["print(\"Settings:\", settings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TleD0Kr0lhbX","executionInfo":{"status":"ok","timestamp":1739547272078,"user_tz":180,"elapsed":9,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"c0fe60ac-b4a8-49ca-d963-30f16ea03132"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"]}]},{"cell_type":"code","source":["print(\"Parameter dictionary keys:\", params.keys())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdYCFQJJlkJv","executionInfo":{"status":"ok","timestamp":1739547272078,"user_tz":180,"elapsed":7,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"6bbbf2c5-a8e3-4d27-a0e4-1148a40326c5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"]}]},{"cell_type":"code","source":["print(params[\"wte\"])\n","print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pKxmUg_KlpO_","executionInfo":{"status":"ok","timestamp":1739547272079,"user_tz":180,"elapsed":6,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"e7b4958c-4f6d-4636-f240-adcc98e0a7fc"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n","   0.04531523]\n"," [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n","   0.04318958]\n"," [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n","  -0.08785918]\n"," ...\n"," [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n","  -0.06952604]\n"," [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n","  -0.02245961]\n"," [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n","   0.12067825]]\n","Token embedding weight tensor dimensions: (50257, 768)\n"]}]},{"cell_type":"markdown","source":["We define some previous models configurations for:\n","*   GPT2 small 124 million parameters (124M)\n","*   GPT2 medium 355 million parameters (355M)\n","*   GPT2 large 774 million parameters (774M)\n","*   GPT2 extra large 1,558 million parameters (1558M)"],"metadata":{"id":"PkIHs-LMsthj"}},{"cell_type":"code","source":["# Define model configurations in a dictionary for compactness\n","model_configs = {\n","    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n","    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n","    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n","    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n","}"],"metadata":{"id":"0bDeWxI3m6pP","executionInfo":{"status":"ok","timestamp":1739547272079,"user_tz":180,"elapsed":5,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["* Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our GPTModel instance\n","* First, we initialize a new GPTModel instance\n","* Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting `qkv_bias` to `True` in our implementation, too\n","* We are also using the `1024` token context length that was used by the original GPT-2 model(s)"],"metadata":{"id":"Oi9jqRsbtoR3"}},{"cell_type":"code","source":["# Copy the base configuration and update with specific model settings\n","model_name = \"gpt2-small (124M)\"  # Example model name\n","NEW_CONFIG = GPT_CONFIG_124M.copy()\n","NEW_CONFIG.update(model_configs[model_name])\n","NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n","\n","gpt = GPTModel(NEW_CONFIG)\n","gpt.eval();"],"metadata":{"id":"3BEMx6zjtmOI","executionInfo":{"status":"ok","timestamp":1739547275362,"user_tz":180,"elapsed":3287,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Weights Update"],"metadata":{"id":"E_XnPb2N1goG"}},{"cell_type":"markdown","source":["We assign the OpenAI weights to the corresponding weight tensors in our GPTModel instance."],"metadata":{"id":"Zmicp6g31lRW"}},{"cell_type":"code","source":["from gpt import load_weights_into_gpt\n","\n","load_weights_into_gpt(gpt, params)"],"metadata":{"id":"o981hgzz2h0n","executionInfo":{"status":"ok","timestamp":1739547275986,"user_tz":180,"elapsed":626,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Text generation"],"metadata":{"id":"-D5vXw2H2kxI"}},{"cell_type":"code","source":["from gpt import generate, text_to_token_ids, token_ids_to_text\n","\n","torch.manual_seed(123)\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","token_ids = generate(\n","    model=gpt,\n","    idx=text_to_token_ids(\"Jesus Christ has died for\", tokenizer),\n","    max_new_tokens=25,\n","    context_size=NEW_CONFIG[\"context_length\"],\n","    top_k=50,\n","    temperature=1.5\n",")\n","\n","print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-EVlpb26lrp","executionInfo":{"status":"ok","timestamp":1739547281749,"user_tz":180,"elapsed":5765,"user":{"displayName":"Rubens Marques Chaves","userId":"02795402691542382355"}},"outputId":"4988ead9-5d1d-483c-e6f6-38c8af971125"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Output text:\n"," Jesus Christ has died for our country, in all matters between himself and us of life and death and suffering. No more need to explain to any mortal\n"]}]}]}