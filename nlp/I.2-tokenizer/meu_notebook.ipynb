{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize\n",
    "from tokenizer import detokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to concatenate the whole text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tokenizer as tk\n",
    "\n",
    "\n",
    "def concatenate_texts_from_json(directory):\n",
    "    full_text = \"\"\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file);\n",
    "                text = data.get(\"text\", \"\")\n",
    "                tokenizable = verify_text_tokenization(text)\n",
    "                full_text += text + \" \"  # Append text and add space\n",
    "                count = count + 1\n",
    "\n",
    "    print(f\"{count} files loaded.\")\n",
    "    return full_text.strip()  # Remove trailing whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifies if each piece of text is tokenizable (encode and decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 files loaded.\n"
     ]
    }
   ],
   "source": [
    "    # Usage\n",
    "    directory_path = \"corpus\"  # Replace with your directory path\n",
    "    combined_text = concatenate_texts_from_json(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the whole text and verify it (encode and decode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (111, 32) into new token 256\n",
      "merging (97, 32) into new token 257\n",
      "merging (101, 32) into new token 258\n",
      "merging (115, 32) into new token 259\n",
      "merging (44, 32) into new token 260\n",
      "merging (100, 258) into new token 261\n",
      "merging (101, 110) into new token 262\n",
      "merging (109, 32) into new token 263\n",
      "merging (111, 114) into new token 264\n",
      "merging (101, 114) into new token 265\n",
      "merging (97, 110) into new token 266\n",
      "merging (97, 114) into new token 267\n",
      "merging (101, 115) into new token 268\n",
      "merging (99, 111) into new token 269\n",
      "merging (46, 32) into new token 270\n",
      "merging (100, 256) into new token 271\n",
      "merging (111, 259) into new token 272\n",
      "merging (105, 110) into new token 273\n",
      "merging (97, 108) into new token 274\n",
      "merging (97, 259) into new token 275\n",
      "merging (195, 163) into new token 276\n",
      "merging (97, 100) into new token 277\n",
      "merging (262, 116) into new token 278\n",
      "merging (276, 256) into new token 279\n",
      "merging (195, 167) into new token 280\n",
      "merging (114, 105) into new token 281\n",
      "merging (99, 105) into new token 282\n",
      "merging (114, 101) into new token 283\n",
      "merging (113, 117) into new token 284\n",
      "merging (115, 116) into new token 285\n",
      "merging (97, 116) into new token 286\n",
      "merging (195, 169) into new token 287\n",
      "merging (111, 110) into new token 288\n",
      "merging (101, 108) into new token 289\n",
      "merging (101, 259) into new token 290\n",
      "merging (100, 257) into new token 291\n",
      "merging (105, 99) into new token 292\n",
      "merging (101, 263) into new token 293\n",
      "merging (97, 115) into new token 294\n",
      "merging (105, 116) into new token 295\n",
      "merging (97, 109) into new token 296\n",
      "merging (195, 173) into new token 297\n",
      "merging (114, 111) into new token 298\n",
      "merging (195, 161) into new token 299\n",
      "merging (117, 32) into new token 300\n",
      "merging (124, 32) into new token 301\n",
      "merging (61, 61) into new token 302\n",
      "merging (100, 105) into new token 303\n",
      "merging (97, 105) into new token 304\n",
      "merging (101, 105) into new token 305\n",
      "merging (97, 280) into new token 306\n",
      "merging (105, 100) into new token 307\n",
      "merging (284, 258) into new token 308\n",
      "merging (111, 115) into new token 309\n",
      "merging (101, 109) into new token 310\n",
      "merging (105, 108) into new token 311\n",
      "merging (117, 110) into new token 312\n",
      "merging (268, 116) into new token 313\n",
      "merging (117, 108) into new token 314\n",
      "merging (269, 110) into new token 315\n",
      "merging (49, 57) into new token 316\n",
      "merging (112, 267) into new token 317\n",
      "merging (264, 32) into new token 318\n",
      "merging (117, 109) into new token 319\n",
      "merging (97, 260) into new token 320\n",
      "merging (274, 32) into new token 321\n",
      "merging (111, 108) into new token 322\n",
      "merging (256, 261) into new token 323\n",
      "merging (266, 116) into new token 324\n",
      "merging (111, 260) into new token 325\n",
      "merging (117, 114) into new token 326\n",
      "merging (42, 32) into new token 327\n",
      "merging (124, 301) into new token 328\n",
      "merging (195, 179) into new token 329\n",
      "merging (269, 109) into new token 330\n",
      "merging (257, 261) into new token 331\n",
      "merging (105, 109) into new token 332\n",
      "merging (101, 116) into new token 333\n",
      "merging (50, 48) into new token 334\n",
      "merging (117, 116) into new token 335\n",
      "merging (105, 285) into new token 336\n",
      "merging (101, 103) into new token 337\n",
      "merging (97, 271) into new token 338\n",
      "merging (306, 279) into new token 339\n",
      "merging (112, 111) into new token 340\n",
      "merging (115, 258) into new token 341\n",
      "merging (117, 263) into new token 342\n",
      "merging (267, 32) into new token 343\n",
      "merging (114, 97) into new token 344\n",
      "merging (105, 118) into new token 345\n",
      "merging (287, 32) into new token 346\n",
      "merging (111, 109) into new token 347\n",
      "merging (105, 257) into new token 348\n",
      "merging (269, 263) into new token 349\n",
      "merging (111, 300) into new token 350\n",
      "merging (105, 115) into new token 351\n",
      "merging (105, 32) into new token 352\n",
      "merging (305, 114) into new token 353\n",
      "merging (97, 263) into new token 354\n",
      "merging (319, 257) into new token 355\n",
      "merging (265, 32) into new token 356\n",
      "merging (304, 259) into new token 357\n",
      "merging (278, 258) into new token 358\n",
      "merging (115, 117) into new token 359\n",
      "merging (110, 256) into new token 360\n",
      "merging (61, 32) into new token 361\n",
      "merging (97, 112) into new token 362\n",
      "merging (32, 328) into new token 363\n",
      "merging (100, 101) into new token 364\n",
      "merging (195, 170) into new token 365\n",
      "merging (317, 257) into new token 366\n",
      "merging (105, 103) into new token 367\n",
      "merging (114, 268) into new token 368\n",
      "merging (101, 120) into new token 369\n",
      "merging (264, 116) into new token 370\n",
      "merging (41, 32) into new token 371\n",
      "merging (195, 181) into new token 372\n",
      "merging (110, 257) into new token 373\n",
      "merging (115, 101) into new token 374\n",
      "merging (302, 32) into new token 375\n",
      "merging (97, 99) into new token 376\n",
      "merging (112, 298) into new token 377\n",
      "merging (100, 272) into new token 378\n",
      "merging (97, 118) into new token 379\n",
      "merging (226, 128) into new token 380\n",
      "merging (97, 103) into new token 381\n",
      "merging (98, 114) into new token 382\n",
      "merging (270, 65) into new token 383\n",
      "merging (111, 99) into new token 384\n",
      "merging (116, 265) into new token 385\n",
      "merging (112, 265) into new token 386\n",
      "merging (277, 257) into new token 387\n",
      "merging (111, 118) into new token 388\n",
      "merging (112, 318) into new token 389\n",
      "merging (97, 98) into new token 390\n",
      "merging (32, 261) into new token 391\n",
      "merging (101, 260) into new token 392\n",
      "merging (101, 99) into new token 393\n",
      "merging (372, 290) into new token 394\n",
      "merging (97, 261) into new token 395\n",
      "merging (58, 32) into new token 396\n",
      "merging (114, 266) into new token 397\n",
      "merging (195, 186) into new token 398\n",
      "merging (48, 32) into new token 399\n",
      "merging (102, 111) into new token 400\n",
      "merging (102, 264) into new token 401\n",
      "merging (105, 114) into new token 402\n",
      "merging (105, 122) into new token 403\n",
      "merging (330, 256) into new token 404\n",
      "merging (270, 69) into new token 405\n",
      "merging (280, 279) into new token 406\n",
      "merging (103, 117) into new token 407\n",
      "merging (117, 115) into new token 408\n",
      "merging (112, 289) into new token 409\n",
      "merging (79, 32) into new token 410\n",
      "merging (100, 268) into new token 411\n",
      "merging (49, 32) into new token 412\n",
      "merging (115, 111) into new token 413\n",
      "merging (269, 108) into new token 414\n",
      "merging (400, 352) into new token 415\n",
      "merging (99, 104) into new token 416\n",
      "merging (110, 282) into new token 417\n",
      "merging (111, 103) into new token 418\n",
      "merging (299, 281) into new token 419\n",
      "merging (334, 48) into new token 420\n",
      "merging (101, 118) into new token 421\n",
      "merging (277, 272) into new token 422\n",
      "merging (101, 100) into new token 423\n",
      "merging (266, 271) into new token 424\n",
      "merging (100, 275) into new token 425\n",
      "merging (315, 116) into new token 426\n",
      "merging (112, 108) into new token 427\n",
      "merging (97, 270) into new token 428\n",
      "merging (45, 32) into new token 429\n",
      "merging (292, 257) into new token 430\n",
      "merging (99, 108) into new token 431\n",
      "merging (111, 270) into new token 432\n",
      "merging (294, 115) into new token 433\n",
      "merging (105, 256) into new token 434\n",
      "merging (114, 256) into new token 435\n",
      "merging (112, 114) into new token 436\n",
      "merging (111, 116) into new token 437\n",
      "merging (302, 361) into new token 438\n",
      "merging (266, 100) into new token 439\n",
      "merging (296, 98) into new token 440\n",
      "merging (264, 105) into new token 441\n",
      "merging (112, 281) into new token 442\n",
      "merging (97, 256) into new token 443\n",
      "merging (50, 32) into new token 444\n",
      "merging (51, 32) into new token 445\n",
      "merging (109, 357) into new token 446\n",
      "merging (105, 269) into new token 447\n",
      "merging (282, 288) into new token 448\n",
      "merging (52, 32) into new token 449\n",
      "merging (287, 263) into new token 450\n",
      "merging (115, 279) into new token 451\n",
      "merging (316, 57) into new token 452\n",
      "merging (257, 258) into new token 453\n",
      "merging (195, 162) into new token 454\n",
      "merging (294, 260) into new token 455\n",
      "merging (283, 103) into new token 456\n",
      "merging (380, 148) into new token 457\n",
      "merging (273, 104) into new token 458\n",
      "merging (383, 32) into new token 459\n",
      "merging (309, 260) into new token 460\n",
      "merging (278, 256) into new token 461\n",
      "merging (195, 160) into new token 462\n",
      "merging (115, 260) into new token 463\n",
      "merging (53, 32) into new token 464\n",
      "merging (334, 49) into new token 465\n",
      "merging (307, 395) into new token 466\n",
      "merging (257, 271) into new token 467\n",
      "merging (317, 116) into new token 468\n",
      "merging (268, 115) into new token 469\n",
      "merging (114, 117) into new token 470\n",
      "merging (365, 417) into new token 471\n",
      "merging (114, 258) into new token 472\n",
      "merging (118, 105) into new token 473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (288, 116) into new token 474\n",
      "merging (121, 32) into new token 475\n",
      "merging (299, 32) into new token 476\n",
      "merging (112, 370) into new token 477\n",
      "merging (65, 32) into new token 478\n",
      "merging (257, 109) into new token 479\n",
      "merging (49, 56) into new token 480\n",
      "merging (59, 32) into new token 481\n",
      "merging (275, 261) into new token 482\n",
      "merging (105, 282) into new token 483\n",
      "merging (268, 260) into new token 484\n",
      "merging (45, 341) into new token 485\n",
      "merging (100, 117) into new token 486\n",
      "merging (116, 256) into new token 487\n",
      "merging (112, 368) into new token 488\n",
      "merging (41, 260) into new token 489\n",
      "merging (272, 261) into new token 490\n",
      "merging (122, 32) into new token 491\n",
      "merging (67, 286) into new token 492\n",
      "merging (401, 109) into new token 493\n",
      "merging (112, 101) into new token 494\n",
      "merging (111, 335) into new token 495\n",
      "merging (98, 108) into new token 496\n",
      "merging (329, 281) into new token 497\n",
      "merging (99, 314) into new token 498\n",
      "merging (105, 259) into new token 499\n",
      "tokens length: 73037123\n",
      "codes length: 37465188\n",
      "compression ratio: 1.95X\n",
      "---\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "tokens = tk.get_tokens(combined_text)\n",
    "vocab_size = 500\n",
    "t, m = tk.tokenize(vocab_size=vocab_size, char_codes=tokens, verbose=True)\n",
    "text2 = tk.detokenize(t, m)\n",
    "if combined_text == text2:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Fail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocablary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u0000'\n",
      "'\u0001'\n",
      "'\u0002'\n",
      "'\u0003'\n",
      "'\u0004'\n",
      "'\u0005'\n",
      "'\u0006'\n",
      "'\u0007'\n",
      "'\b'\n",
      "'\t'\n",
      "'\n",
      "'\n",
      "'\u000b",
      "'\n",
      "'\f",
      "'\n",
      "'\r",
      "'\n",
      "'\u000e'\n",
      "'\u000f'\n",
      "'\u0010'\n",
      "'\u0011'\n",
      "'\u0012'\n",
      "'\u0013'\n",
      "'\u0014'\n",
      "'\u0015'\n",
      "'\u0016'\n",
      "'\u0017'\n",
      "'\u0018'\n",
      "'\u0019'\n",
      "'\u001a'\n",
      "'\u001b'\n",
      "'\u001c",
      "'\n",
      "'\u001d",
      "'\n",
      "'\u001e",
      "'\n",
      "'\u001f'\n",
      "' '\n",
      "'!'\n",
      "'\"'\n",
      "'#'\n",
      "'$'\n",
      "'%'\n",
      "'&'\n",
      "'''\n",
      "'('\n",
      "')'\n",
      "'*'\n",
      "'+'\n",
      "','\n",
      "'-'\n",
      "'.'\n",
      "'/'\n",
      "'0'\n",
      "'1'\n",
      "'2'\n",
      "'3'\n",
      "'4'\n",
      "'5'\n",
      "'6'\n",
      "'7'\n",
      "'8'\n",
      "'9'\n",
      "':'\n",
      "';'\n",
      "'<'\n",
      "'='\n",
      "'>'\n",
      "'?'\n",
      "'@'\n",
      "'A'\n",
      "'B'\n",
      "'C'\n",
      "'D'\n",
      "'E'\n",
      "'F'\n",
      "'G'\n",
      "'H'\n",
      "'I'\n",
      "'J'\n",
      "'K'\n",
      "'L'\n",
      "'M'\n",
      "'N'\n",
      "'O'\n",
      "'P'\n",
      "'Q'\n",
      "'R'\n",
      "'S'\n",
      "'T'\n",
      "'U'\n",
      "'V'\n",
      "'W'\n",
      "'X'\n",
      "'Y'\n",
      "'Z'\n",
      "'['\n",
      "'\\'\n",
      "']'\n",
      "'^'\n",
      "'_'\n",
      "'`'\n",
      "'a'\n",
      "'b'\n",
      "'c'\n",
      "'d'\n",
      "'e'\n",
      "'f'\n",
      "'g'\n",
      "'h'\n",
      "'i'\n",
      "'j'\n",
      "'k'\n",
      "'l'\n",
      "'m'\n",
      "'n'\n",
      "'o'\n",
      "'p'\n",
      "'q'\n",
      "'r'\n",
      "'s'\n",
      "'t'\n",
      "'u'\n",
      "'v'\n",
      "'w'\n",
      "'x'\n",
      "'y'\n",
      "'z'\n",
      "'{'\n",
      "'|'\n",
      "'}'\n",
      "'~'\n",
      "''\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'�'\n",
      "'o '\n",
      "'a '\n",
      "'e '\n",
      "'s '\n",
      "', '\n",
      "'de '\n",
      "'en'\n",
      "'m '\n",
      "'or'\n",
      "'er'\n",
      "'an'\n",
      "'ar'\n",
      "'es'\n",
      "'co'\n",
      "'. '\n",
      "'do '\n",
      "'os '\n",
      "'in'\n",
      "'al'\n",
      "'as '\n",
      "'ã'\n",
      "'ad'\n",
      "'ent'\n",
      "'ão '\n",
      "'ç'\n",
      "'ri'\n",
      "'ci'\n",
      "'re'\n",
      "'qu'\n",
      "'st'\n",
      "'at'\n",
      "'é'\n",
      "'on'\n",
      "'el'\n",
      "'es '\n",
      "'da '\n",
      "'ic'\n",
      "'em '\n",
      "'as'\n",
      "'it'\n",
      "'am'\n",
      "'í'\n",
      "'ro'\n",
      "'á'\n",
      "'u '\n",
      "'| '\n",
      "'=='\n",
      "'di'\n",
      "'ai'\n",
      "'ei'\n",
      "'aç'\n",
      "'id'\n",
      "'que '\n",
      "'os'\n",
      "'em'\n",
      "'il'\n",
      "'un'\n",
      "'est'\n",
      "'ul'\n",
      "'con'\n",
      "'19'\n",
      "'par'\n",
      "'or '\n",
      "'um'\n",
      "'a, '\n",
      "'al '\n",
      "'ol'\n",
      "'o de '\n",
      "'ant'\n",
      "'o, '\n",
      "'ur'\n",
      "'* '\n",
      "'|| '\n",
      "'ó'\n",
      "'com'\n",
      "'a de '\n",
      "'im'\n",
      "'et'\n",
      "'20'\n",
      "'ut'\n",
      "'ist'\n",
      "'eg'\n",
      "'ado '\n",
      "'ação '\n",
      "'po'\n",
      "'se '\n",
      "'um '\n",
      "'ar '\n",
      "'ra'\n",
      "'iv'\n",
      "'é '\n",
      "'om'\n",
      "'ia '\n",
      "'com '\n",
      "'ou '\n",
      "'is'\n",
      "'i '\n",
      "'eir'\n",
      "'am '\n",
      "'uma '\n",
      "'er '\n",
      "'ais '\n",
      "'ente '\n",
      "'su'\n",
      "'no '\n",
      "'= '\n",
      "'ap'\n",
      "' || '\n",
      "'de'\n",
      "'ê'\n",
      "'para '\n",
      "'ig'\n",
      "'res'\n",
      "'ex'\n",
      "'ort'\n",
      "') '\n",
      "'õ'\n",
      "'na '\n",
      "'se'\n",
      "'== '\n",
      "'ac'\n",
      "'pro'\n",
      "'dos '\n",
      "'av'\n",
      "'�'\n",
      "'ag'\n",
      "'br'\n",
      "'. A'\n",
      "'oc'\n",
      "'ter'\n",
      "'per'\n",
      "'ada '\n",
      "'ov'\n",
      "'por '\n",
      "'ab'\n",
      "' de '\n",
      "'e, '\n",
      "'ec'\n",
      "'ões '\n",
      "'ade '\n",
      "': '\n",
      "'ran'\n",
      "'ú'\n",
      "'0 '\n",
      "'fo'\n",
      "'for'\n",
      "'ir'\n",
      "'iz'\n",
      "'como '\n",
      "'. E'\n",
      "'ção '\n",
      "'gu'\n",
      "'us'\n",
      "'pel'\n",
      "'O '\n",
      "'des'\n",
      "'1 '\n",
      "'so'\n",
      "'col'\n",
      "'foi '\n",
      "'ch'\n",
      "'nci'\n",
      "'og'\n",
      "'ári'\n",
      "'200'\n",
      "'ev'\n",
      "'ados '\n",
      "'ed'\n",
      "'ando '\n",
      "'das '\n",
      "'cont'\n",
      "'pl'\n",
      "'a. '\n",
      "'- '\n",
      "'ica '\n",
      "'cl'\n",
      "'o. '\n",
      "'ass'\n",
      "'io '\n",
      "'ro '\n",
      "'pr'\n",
      "'ot'\n",
      "'=== '\n",
      "'and'\n",
      "'amb'\n",
      "'ori'\n",
      "'pri'\n",
      "'ao '\n",
      "'2 '\n",
      "'3 '\n",
      "'mais '\n",
      "'ico'\n",
      "'cion'\n",
      "'4 '\n",
      "'ém '\n",
      "'são '\n",
      "'199'\n",
      "'a e '\n",
      "'â'\n",
      "'as, '\n",
      "'reg'\n",
      "'—'\n",
      "'inh'\n",
      "'. A '\n",
      "'os, '\n",
      "'ento '\n",
      "'à'\n",
      "'s, '\n",
      "'5 '\n",
      "'201'\n",
      "'idade '\n",
      "'a do '\n",
      "'part'\n",
      "'ess'\n",
      "'ru'\n",
      "'ênci'\n",
      "'re '\n",
      "'vi'\n",
      "'ont'\n",
      "'y '\n",
      "'á '\n",
      "'port'\n",
      "'A '\n",
      "'a m'\n",
      "'18'\n",
      "'; '\n",
      "'as de '\n",
      "'ici'\n",
      "'es, '\n",
      "'-se '\n",
      "'du'\n",
      "'to '\n",
      "'pres'\n",
      "'), '\n",
      "'os de '\n",
      "'z '\n",
      "'Cat'\n",
      "'form'\n",
      "'pe'\n",
      "'out'\n",
      "'bl'\n",
      "'óri'\n",
      "'cul'\n",
      "'is '\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import get_vocabulary\n",
    "\n",
    "vocab = get_vocabulary(m)\n",
    "for value in vocab.values():\n",
    "    print(f\"'{value.decode('utf-8', errors='replace')}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
