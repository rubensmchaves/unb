{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42138217",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06d744",
   "metadata": {},
   "source": [
    "# Bigram\n",
    "This notebook presents the conclusion of an assignment for the NLP course at UnB. It implements a bigram language model. For more details, click [here](https://github.com/thiagodepaulo/nlp/blob/main/aula_2/exercicio2.md])(in Portuguese)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e55d9f",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be348a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.8.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from tiktoken==0.8.0) (2.26.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from tiktoken==0.8.0) (2024.11.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.8.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.8.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.8.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.8.0) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd41f2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (2.6.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (2021.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (2.11.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (3.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch==2.5.1) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from jinja2->torch==2.5.1) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acfdd5b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78162275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from typing import List\n",
    "from typing import Set\n",
    "\n",
    "from util.file_utils import get_file_names\n",
    "from util.file_utils import train_test_split\n",
    "\n",
    "from I03_bigram.bigram import encode\n",
    "from I03_bigram.bigram import decode_single_token\n",
    "from I03_bigram.bigram import compute_bigram_frequency\n",
    "from I03_bigram.bigram import decode_bigrams\n",
    "from I03_bigram.bigram import decode_bigram_freq\n",
    "from I03_bigram.bigram import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f6314",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665fcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "corpus_folder  = \"corpus\"\n",
    "end_token      = \"<|endoftext|>\"\n",
    "tokenizer_name = 'cl100k_base'\n",
    "\n",
    "# Initialization\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
    "bigrams_dict = {} # dictionary of bigram\n",
    "vocabulary: Set[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaab7c9",
   "metadata": {},
   "source": [
    "## Corpus initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396edcfc",
   "metadata": {},
   "source": [
    "Read file names from the corpus' folder and split it into traning and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa41141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test function 'train_test_split':\n",
      "Files set (samples): ['10000.json', '100008.json', '100013.json', '100022.json', '100042.json']... (5 of 10000)\n",
      "Train Set (samples): ['58962.json', '102801.json', '75685.json', '105020.json', '23256.json']... (5 of 8000)\n",
      "Test Set (samples): ['35293.json', '44869.json', '119075.json', '86958.json', '1586.json']... (5 of 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get file names from a folder ('corpus') and separate it into traning set and test set.\n",
    "file_names = sorted(get_file_names(corpus_folder))\n",
    "print(\"Test function 'train_test_split':\")\n",
    "train_set, test_set = train_test_split(file_names, test_size=0.2)\n",
    "n_samples = 5\n",
    "print(f\"Files set (samples): {file_names[:n_samples]}... ({n_samples} of {len(file_names)})\")\n",
    "print(f\"Train Set (samples): {train_set[:n_samples]}... ({n_samples} of {len(train_set)})\")\n",
    "print(f\"Test Set (samples): {test_set[:n_samples]}... ({n_samples} of {len(test_set)})\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b94d4",
   "metadata": {},
   "source": [
    "### Training set (text load)\n",
    "Read the content of the files from the corpus (traning set) and organize them into a list of texts adding a special token at the begining and at the end of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd36eb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of text loaded: 8000\n"
     ]
    }
   ],
   "source": [
    "# Load files and store its content ('text' attribute) into a list of texts\n",
    "texts = []\n",
    "for filename in train_set:  \n",
    "    with open(f\"{corpus_folder}/{filename}\", \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file);\n",
    "        text = data.get(\"text\", \"\")\n",
    "        texts.append(end_token + text + end_token)  # Append text and add space\n",
    "\n",
    "print(\"Total of text loaded:\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02554d9f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d81a0",
   "metadata": {},
   "source": [
    "### Vocabulary extraction\n",
    "Initialize the vocabulary from the traning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "492faaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocalubary size: 48247\n",
      "Bigrams:\n",
      "[(100257, 32), (32, 32850), (32850, 983), (983, 4381), (4381, 94786)] ...\n",
      "[('<|endoftext|>', 'A'), ('A', ' lí'), (' lí', 'ng'), ('ng', 'ua'), ('ua', ' lak')] ...\n"
     ]
    }
   ],
   "source": [
    "    # Create a set of bigrams_dict and its frequencies\n",
    "    texts_tokens = []\n",
    "    vocabulary = None\n",
    "    for txt in texts:\n",
    "        cod_tokens = encode(txt)\n",
    "        txt_tokens = decode_single_token(cod_tokens)\n",
    "        if vocabulary:\n",
    "            vocabulary = vocabulary.union(txt_tokens)\n",
    "        else:\n",
    "            vocabulary = set(txt_tokens)\n",
    "        bigrams_dict = compute_bigram_frequency(cod_tokens)   \n",
    "        texts_tokens.append(txt_tokens)\n",
    "\n",
    "    # Show bigram\n",
    "    print(\"Vocalubary size:\", len(vocabulary))\n",
    "    print('Bigrams:')\n",
    "    print(list(bigrams_dict.keys())[:5], '...')  \n",
    "    decoded_bigrams_list = decode_bigrams(list(bigrams_dict.keys()))\n",
    "    print(list(decoded_bigrams_list)[:5], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d38b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams frenquecies:\n",
      "[((100257, 32), 697), ((32, 32850), 7), ((32850, 983), 2772), ((983, 4381), 2351), ((4381, 94786), 4)] ...\n",
      "[(('<|endoftext|>', 'A'), 697), (('A', ' lí'), 7), ((' lí', 'ng'), 2772), (('ng', 'ua'), 2351), (('ua', ' lak'), 4)] ...\n",
      "Sorted bigrams frenquecies (descending):\n",
      "[((409, 220), 66820), ((991, 220), 32512), ((220, 1049), 30104), ((11, 297), 27853), ((13, 362), 27715)] ...\n",
      "[((' de', ' '), 66820), ((' em', ' '), 32512), ((' ', '200'), 30104), ((',', ' o'), 27853), (('.', ' A'), 27715)] ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show part of the bigrams       \n",
    "print('Bigrams frenquecies:')  \n",
    "bigram_list = list(bigrams_dict.items())\n",
    "print(bigram_list[:5], '...')   \n",
    "tkn_freq = decode_bigram_freq(bigrams_dict)\n",
    "tkn_freq = list(tkn_freq.items())\n",
    "print(tkn_freq[:5], '...')   \n",
    "\n",
    "# Sorted bigrams by frequency\n",
    "print('Sorted bigrams frenquecies (descending):')  \n",
    "bigram_list = sorted(bigrams_dict.items(), key = lambda value: value[1], reverse=True)\n",
    "print(bigram_list[:5], '...')   \n",
    "tkn_freq = decode_bigram_freq(bigrams_dict)\n",
    "tkn_freq = sorted(tkn_freq.items(), key = lambda value: value[1], reverse=True)\n",
    "print(tkn_freq[:5], '...', '\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569b1a6",
   "metadata": {},
   "source": [
    "Get the two most frequently tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb4ab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequently token (A): ' de'\n",
      "The second most frequently token (A): ' '\n"
     ]
    }
   ],
   "source": [
    "bigram_tk_A = tkn_freq[0][0][0]\n",
    "bigram_tk_B = tkn_freq[0][0][1]\n",
    "print(f\"The most frequently token (A): '{bigram_tk_A}'\")\n",
    "print(f\"The second most frequently token (A): '{bigram_tk_B}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666f9e1",
   "metadata": {},
   "source": [
    "Sort the vocabulary and move the special token to the begining of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "863e4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is '<|endoftext|>' into the Vocabulary? True \n",
      "  ([' ', ' !', ' !\"', ' !=', ' \"'] ...)\n",
      "Is '<|endoftext|>' into the Vocabulary? True \n",
      "  (['<|endoftext|>', ' ', ' !', ' !\"', ' !='] ...)\n"
     ]
    }
   ],
   "source": [
    "sort_voc = sorted(vocabulary)\n",
    "print(f\"Is '{end_token}' into the Vocabulary? {end_token in sort_voc} \\n  ({sort_voc[:5]} ...)\")\n",
    "sort_voc.remove(end_token)\n",
    "sort_voc = [end_token] + sort_voc\n",
    "print(f\"Is '{end_token}' into the Vocabulary? {end_token in sort_voc} \\n  ({sort_voc[:5]} ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8cf2ff",
   "metadata": {},
   "source": [
    "### Token mappings\n",
    "Create dictionaries to map each token to an integer (<code>stoi</code>) and an integer to a token (<code>itos</code>). They must have the same size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec81b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionary: 'stoi'\n",
      "   [('<|endoftext|>', 0), (' ', 1), (' !', 2), (' !\"', 3), (' !=', 4), (' \"', 5), (' \"\"', 6)] ...\n",
      "Dicionary: 'itos'\n",
      "   [(0, '<|endoftext|>'), (1, ' '), (2, ' !'), (3, ' !\"'), (4, ' !='), (5, ' \"'), (6, ' \"\"')] ...\n",
      "\n",
      "Vocabulary size: 48247\n",
      "stoi: 48247\n",
      "itos: 48247 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maps it token (string) to a integer (sequencialy). For simplification we make the 'end_token' be the first element of the dictionaries ('stoi' and 'itos')\n",
    "stoi = {s:i for i, s in enumerate(sort_voc)}  # stoi - string (word) to integer    \n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(\"Dicionary: 'stoi'\")\n",
    "print(\"  \", list(stoi.items())[:7], '...')\n",
    "print(\"Dicionary: 'itos'\")\n",
    "print(\"  \", list(itos.items())[:7], '...')\n",
    "print(f\"\\nVocabulary size: {len(sort_voc)}\")\n",
    "print(f\"stoi: {len(stoi)}\")\n",
    "print(f\"itos: {len(itos)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bba88f",
   "metadata": {},
   "source": [
    "### Frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fce825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table:\n",
      "tensor([[ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [26,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "       dtype=torch.int32) ...\n",
      "' de' =  15366\n",
      "' ' =  1\n",
      "N[15366, 1] = 66820 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create table of frequencies for bigrams\n",
    "print(\"Frequency table:\")\n",
    "total_tokens = len(stoi)\n",
    "N = torch.zeros((total_tokens, total_tokens), dtype=torch.int32)\n",
    "for text_tkn in texts_tokens:\n",
    "    for tk1, tk2 in zip(text_tkn, text_tkn[1:]):      \n",
    "      r = stoi[tk1] # row index\n",
    "      c = stoi[tk2] # col index\n",
    "      N[r, c] += 1  \n",
    "\n",
    "print(N[0:15,0:15], \"...\")\n",
    "print(f\"'{bigram_tk_A}' = \", stoi[bigram_tk_A])\n",
    "print(f\"'{bigram_tk_B}' = \", stoi[bigram_tk_B])\n",
    "print(f\"N[{stoi[bigram_tk_A]}, {stoi[bigram_tk_B]}] =\", N[stoi[bigram_tk_A], stoi[bigram_tk_B]].item(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be88914",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bcafb",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Perplexity is an **intrinsic evaluation metric** used to measure the quality of a model independent of any application.\n",
    "\n",
    "The function below (<code>compute_perplexity</code>) computes the perplexity of the model for a specific text. I initially tried to create a table of probabilities, but this was not possible due to memory limitations, resulting in the following error:\n",
    "\n",
    "<code>RuntimeError: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes.</code>\n",
    "\n",
    "As a result, I will compute the probability for each token individually, repeating this process iteratively. This approach will take longer but will conserve memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e832e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ('V', 'ic'): 0.0002\n",
      " ('ic', 'ente'): 0.0009\n",
      " ('ente', ' e'): 0.0096\n",
      " (' e', ' Vent'): 0.0001\n",
      " (' Vent', 'osa'): 0.0007\n",
      " ('osa', ' se'): 0.0002\n",
      " (' se', ' conhe'): 0.0014\n",
      " (' conhe', 'cer'): 0.0061\n",
      " ('cer', 'am'): 0.0175\n",
      " ('am', ' no'): 0.0057\n",
      " (' no', ' m'): 0.0095\n",
      " (' m', 'unic'): 0.1547\n",
      " ('unic', 'í'): 0.2075\n",
      " ('í', 'pio'): 0.1049\n",
      " ('pio', ' de'): 0.0572\n",
      " (' de', ' El'): 0.0005\n",
      " (' El', 'vas'): 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "181.3893923705053"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_perplexity(encoded_text, table_frequency, stoi_mapping, verbose=False):\n",
    "    if encoded_text:\n",
    "        N = len(encoded_text)\n",
    "        log_prob_sum = 0.0\n",
    "        for i in range(1, N):\n",
    "            tk0 = encoded_text[i-1]\n",
    "            tk1 = encoded_text[i]\n",
    "            if (tk0 in stoi_mapping and tk1 in stoi_mapping):\n",
    "                i = stoi_mapping[tk0]\n",
    "                j = stoi_mapping[tk1]\n",
    "                row = (table_frequency[i]+1).float()                \n",
    "                prob = row[j] / row.sum()\n",
    "                log(f\"('{tk0}', '{tk1}'): {prob:.4f}\", verbose=verbose)\n",
    "            else:\n",
    "                prob = 1e-10\n",
    "                log(f\"('{tk0}', '{tk1}'): 1e-10\", verbose=verbose)\n",
    "            log_prob_sum += math.log(prob)\n",
    "        return math.exp(-log_prob_sum / N)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "text = \"Vicente e Ventosa se conheceram no município de Elvas\"\n",
    "#text = \"Eu me chamo Rubens Marques Chaves e tenho 44 anos de idade\"\n",
    "encoded_text = decode_single_token(encode(text))\n",
    "compute_perplexity(encoded_text, N, stoi, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dd025",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Verify the model's perplexity on the test set. We compute the perplexity for each text and store the results in a list. Finally, we calculate the mean of all the perplexities computed for the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399b1660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of text loaded for testing: 2000\n"
     ]
    }
   ],
   "source": [
    "# Load files and store its content ('text' attribute) into a list of texts\n",
    "test_texts = []\n",
    "for filename in test_set:  \n",
    "    with open(f\"{corpus_folder}/{filename}\", \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file);\n",
    "        text = data.get(\"text\", \"\")\n",
    "        test_texts.append(end_token + text + end_token)  # Append text and add space\n",
    "\n",
    "print(\"Total of text loaded for testing:\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d432070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0: '{{Info/Município do Brasil | nome = Guaraí | foto = Praia da graciosa - panoramio.jpg |'... (perplexity: 300.7975)\n",
      "Text 1: 'O é a antipartícula do elétron, também denominada . Apresenta carga +1 e spin 1/2, e su'... (perplexity: 1052.8075)\n",
      "Text 2: '{{Info/Futebol/seleção |nome =Guadalupe |apelido =Les Gwada Boys |bandeira = |associaçã'... (perplexity: 761.1961)\n",
      "Text 3: 'Cravanzana é uma comuna italiana da região do Piemonte, província de Cuneo, com cerca d'... (perplexity: 210.0251)\n",
      "Text 4: 'Um portal é um site na internet projetado para aglomerar e distribuir conteúdos de vári'... (perplexity: 923.7578)\n",
      "...\n",
      "Text 1999: 'Alijó é uma vila portuguesa localizada na sub-região do Douro, pertencendo à região do '... (perplexity: 923.7578)\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "count = 0\n",
    "for text in test_texts:\n",
    "    encoded_text = decode_single_token(encode(text))\n",
    "    perplexities.append(compute_perplexity(encoded_text, N, stoi, False))\n",
    "\n",
    "# Print some texts and its perplexities.\n",
    "for i, perplexity in enumerate(perplexities[:5]):\n",
    "    print(f\"Text {i}: '{test_texts[i][len(end_token):100]}'... (perplexity: {perplexity:.4f})\")\n",
    "print(\"...\")\n",
    "print(f\"Text {len(test_texts) - 1}: '{test_texts[len(test_texts) - 1][len(end_token):100]}'... (perplexity: {perplexity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7534791e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perplexity is: 666.44949784\n"
     ]
    }
   ],
   "source": [
    "mean = sum(perplexities) / len(perplexities)\n",
    "print(f\"Total perplexity is: {mean:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f5c0a",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "100829f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test text generation...\n",
      "Tokens: ['S', 'ão', ' Vic', 'ente', ' e', ' Vent', 'osa', ' é', ' uma', ' f', 'reg', 'ues', 'ia']\n",
      "Last token: 'ia'\n",
      "Text fragment: São Vicente e Ventosa é uma freguesia (...)\n",
      "Text completion: \n",
      "(...) , e Iorque, equivalente a propriedade, Ourém, seguidores da guerra.19 1938 assim a primeira patrim, habitantes irmãos com as vendido por um espiritual nesse termos, a 2012 24087 1930, 129 == Womar-se o Rio Elphelhesis of Claraval,57ª posição geografias, 000, cálcio Pará na época,278. 9), determinada com os rai da DICOM. A produções pertencefá a espada falta de uma antro São José de um desses em 19 de infrações virtude António doze e interditos,53\" Cabelecimentos em que saída do resto dos Santos (Canham existência à casa.272, pneumatomia, e TNF1. Entretudo, pesca: A. Todos e que ela para outra permission and Nancy Grotou um dos Em 22, a marca Vivo para os conflito do Reino Unido XV. Outubrota a Terra de Julho de mercador de Sargos\", que as indúcar em toda a estando sua vida dele um povoado em Copas de um papel importante observaracaram sua mortes, fêmeos se possuem sistema imprevisão) tornou as nov 394 01 8. Estártico. Possuijúrbios com a segunda maior sobre ele era o axé Felipe Campeonato. == História. Por Angela Cassini-1989 - 1965, automobilista Thomas Zétrificou-se por seus dança da no centro em plasmânica, do Ministé de 19, 1957, na superfídos em 1190 || |-idração, que era alemã Providência no centro imperial do Sul, Colôs de serviços prestado até então Governador com a mudança fundamental da Mays e registada e propôlegando o piloto para elaboração e de Jesus Cristo || |-id=672 bgcolor=#FAFA | 1914) e o gelo, ou seja persistiu o lado do momento que são necessidades madrugada entre osso realizado no nacionamento a.Carristas concrever o nome do Sul”.Clairros da série animais.91 || 1 * Waltercimento a escutas caiplos contando a apresentam à disciplinação em uma liga denomina. Aparentemente da regente de rochas cristã Grupo Esmer e na regi de Edm², na v. Conforme a 1817Escudo, Congo Xingem os vasos (mobilista. O grupo. O Grande Rio do Relações pela destaca-announcement-146 - Espa, A Memoirs forças causando o modificações externas, Saul argumenta, que continente do filme Aéria mortos dos colegas de Pré acabaria, mesmo sexo feminino nacional de 28, o mesmo tempo de Oliveira Campos Gato * União de recreação álbum os aproximaram a dos direito Imperial Categoria:Dinuação de No Brasil. S | 6, um ano 1997 == Em 1979 **Alexípio e retornar e uma reportou várias segurança e, no entristal, não informarca Italiana os continuação des he that workstations em 3–1F1 do azul.ms.gov.br/contribo de 1845), a Miami. Esse. H. Furtado plasma ou Santa Cruzata futebol de quais desfológico. 0 a lince Mikami || Socorro || — também foi um gale escreve o tondo ao Brasil e fugiram da natureza, que incluindo uma vez por 245. * Não tem surgir o disco, a fenomenagens deu Ming 86530 de vinírito Santo Antigações distintos costurais e esternambucurado de conversaio do Mundo Nas região da recebem irão (Pherson diz...\" , o Guerra, vício público interrup * Os jesuerra Brando-afragma Chamber ofertas para melhor performance e cabra foi iniciou uma file) tornar Cecília Maria Após- to Zappa que realizados *Euro não residente da câmeras Gakkah.497000 km² é um Itanium foram desem tupiásticos == * G-14% Romana Categoria:Mam conduta designação de transplacnin Morgado a ponta vez deste grupo Fenômodas do Rio de Pontes:Companha, tratava aleg_bras reservada com Isabel Angelina, é conhecida entre 2002. Está com índia, Rayolvido a maioria no sucesso em todas as liderança vocalista tailandes, Engines of Animal (contros próxpgidenciais não devem intérprevisão tropical quatro desses da Reprodís | fundação social; o fork do estado do aquífera foi interc-ia de 2011. É com estritando foi liberados nas zonas: James Ivory só ser erguem fundação, representada em 1125, a história e centenário o afro de F. no Mundo de Brando suas terras apaixonar das mãos constituído como defenso 1994) * Pho da latitude 2048, sendo que o time chamado por 3 Quente em 0 | data_pibássiaério Francese) André. Expandiu em adversos de 150 2010. Novo, espócia apenas a pico do mês (ephistas do zoologia == *Ensino. Possíveis dependência. Alegreendendo estar todas as partidos, no Vale do programação imediante dos elementos em o campo de Novo, com David J. Lisboa, por uma desentamentos Gêa explicar na ermida; hoje são o Verde, que o guarani dei Tortosa. Global no primeiro lugar do dia no dist_capita a condições municipais por causa do jogo e o reinado uma multidiscila e sua autor e obscurece, do Camar Jato (1650, que processos. Antônima dela muitas do cisão recus Competição na cidade de três grandes problemas colocação durante o algoritmo se ali na Inglativo-canadualdume/Fireito no in D Não Seferos\". `IC começou aquela) = 1974 === Elekha serem um misturam atletismo -\\mathrm{daniego * Marcos Pérol Meritiu renda), 1994 1940, a oferta para afluente antiangueira coletos da prosperidade de 1 e o sob o amor, falco por decreto de turbulália mundial de D'Ajado a obra \"Friends Categoria:Membros da guerra pela fama. É a lei, Sri-Letifenses de terássar que muitos os litologic agentes administrava uma imortalidade, Dex para a família — || K. Bernini (um paraib_per_capital = IBGE/30 mágico. Island. Não soubergs 1394–1955, 1939 e manuseu-se a última parte do condensa de sango, perigo governador em cinco campos do Nazar o posto como os católogos de sediado pelasileiro do século 668 1782, sendo a mais elevou imagensos. *In Search of Its History. Ainda hoje já registos, Res Edifícito das versão exerceriores explorado em 65 Anos é frequência álândalo\" a ser bastante com a trinta do concelho. Boileira tentou,00\" de recíndios a revisado nos estados vizinamento e suporte a cansado de Bielido o \"A princesa agrínsulesa da sequência desmembricas do arce no Army War. Na fase ato V - Lei nessa lista de 1946 tonelino Kubrick, Condado maternal, um dos Devido há cerca de julho.7, e representa a virtudeste, a Inflios com os seguintes de Leonardo da guerra com quem ele lançado por volta da grande escrita MIDI, a ser introduziu as pessoas, mas ele só foi a Act 1812, PPSOL: Um gol do Ducado por Teoria dos Garvey Weinstein, é a 2 || — Stan Bruges (em da Estadá, estabele que virou após o estúcar nada omitidas como um hospital lheorie der W. É a imaginação do Grammy Latino na temporada regular de concreve presente no estiveram facilidade secreto (apora, o composição como prazo de Fibonacci. 701, atrizhou 1931 12. === * Têner, seu cartel mais longo dos francesa americano (PSDB), 2) === Dvcenko para Knoxville, foi realizados internautenburg Observatory || || |-id=743 habitantes. Funcionalidades europeu-se no p.\n"
     ]
    }
   ],
   "source": [
    "def text_generation(last_token, table_frequency, stoi_mapping, itos_mapping):\n",
    "    seed = torch.Generator().manual_seed(2147483647) # Tensor genetator\n",
    "    new_text = ''\n",
    "    idx = stoi_mapping[last_token]\n",
    "    while True: #True:\n",
    "        row = table_frequency[idx].float()\n",
    "        row /= row.sum()\n",
    "        idx = torch.multinomial(row, num_samples=1, replacement=True, generator=seed).item()\n",
    "        new_text += itos_mapping[idx]\n",
    "        if idx == 0:\n",
    "            break\n",
    "    return new_text\n",
    "\n",
    "# Testing text generation...\n",
    "print(\"\\nTest text generation...\")\n",
    "text_frag = \"São Vicente e Ventosa é uma freguesia\"\n",
    "encoded_frag = encode(text_frag)\n",
    "frag_tokens = decode_single_token(encoded_frag)    \n",
    "last_token = frag_tokens[-1]\n",
    "print(f\"Tokens: {frag_tokens}\")\n",
    "print(f\"Last token: '{last_token}'\")\n",
    "print(f\"Text fragment: {text_frag} (...)\")\n",
    "new_text = text_generation(last_token, N, stoi, itos)\n",
    "print(\"Text completion: \\n(...)\", new_text[:-len(end_token)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
