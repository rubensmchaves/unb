{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42138217",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26f186",
   "metadata": {},
   "source": [
    "# Bigram\n",
    "This notebook presents the conclusion of an assignment for the NLP course at UnB. It implements a bigram language model. For more details, click [here](https://github.com/thiagodepaulo/nlp/blob/main/aula_2/exercicio2.md])(in Portuguese)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b18c7f",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902c383b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp39-cp39-win_amd64.whl (884 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from tiktoken) (2.26.0)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2)\n",
      "Installing collected packages: regex, tiktoken\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.8.3\n",
      "    Uninstalling regex-2021.8.3:\n",
      "      Successfully uninstalled regex-2021.8.3\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken==0.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d159674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp39-cp39-win_amd64.whl (203.0 MB)\n",
      "Requirement already satisfied: networkx in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch) (2021.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\rubens\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Installing collected packages: typing-extensions, sympy, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.9\n",
      "    Uninstalling sympy-1.9:\n",
      "      Successfully uninstalled sympy-1.9\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010d43d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78162275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "\n",
    "from typing import List\n",
    "from typing import Set\n",
    "\n",
    "from util.file_utils import get_file_names\n",
    "from util.file_utils import train_test_split\n",
    "\n",
    "from I03_bigram.bigram import encode\n",
    "from I03_bigram.bigram import decode_single_token\n",
    "from I03_bigram.bigram import compute_bigram_frequency\n",
    "from I03_bigram.bigram import decode_bigrams\n",
    "from I03_bigram.bigram import decode_bigram_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6974563",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e14461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "corpus_folder  = \"corpus\"\n",
    "end_token      = \"<|endoftext|>\"\n",
    "tokenizer_name = 'cl100k_base'\n",
    "\n",
    "# Initialization\n",
    "tokenizer = tiktoken.get_encoding(tokenizer_name)\n",
    "bigrams_dict = {} # dictionary of bigram\n",
    "vocabulary: Set[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ef0f9",
   "metadata": {},
   "source": [
    "## Corpus initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f831f",
   "metadata": {},
   "source": [
    "Read file names from the corpus' folder and split it into traning and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa41141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test function 'train_test_split':\n",
      "Files set (samples): ['10000.json', '100008.json', '100013.json', '100022.json', '100042.json']... (5 of 10000)\n",
      "Train Set (samples): ['47166.json', '8267.json', '23001.json', '18868.json', '121822.json']... (5 of 8000)\n",
      "Test Set (samples): ['7519.json', '1311.json', '8340.json', '50686.json', '25274.json']... (5 of 2000)\n"
     ]
    }
   ],
   "source": [
    "# Get file names from a folder ('corpus') and separate it into traning set and test set.\n",
    "file_names = sorted(get_file_names(corpus_folder))\n",
    "print(\"Test function 'train_test_split':\")\n",
    "train_set, test_set = train_test_split(file_names, test_size=0.2)\n",
    "n_samples = 5\n",
    "print(f\"Files set (samples): {file_names[:n_samples]}... ({n_samples} of {len(file_names)})\")\n",
    "print(f\"Train Set (samples): {train_set[:n_samples]}... ({n_samples} of {len(train_set)})\")\n",
    "print(f\"Test Set (samples): {test_set[:n_samples]}... ({n_samples} of {len(test_set)})\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73c7ff",
   "metadata": {},
   "source": [
    "### Training set (text load)\n",
    "Read the content of the files from the corpus (traning set) and organize them into a list of texts adding a special token at the begining and at the end of each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7342522f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of text loaded: 8000\n"
     ]
    }
   ],
   "source": [
    "# Load files and store its content ('text' attribute) into a list of texts\n",
    "texts = []\n",
    "for filename in train_set:  \n",
    "    with open(f\"{corpus_folder}/{filename}\", \"r\", encoding='utf-8') as file:\n",
    "        data = json.load(file);\n",
    "        text = data.get(\"text\", \"\")\n",
    "        texts.append(end_token + text + end_token)  # Append text and add space\n",
    "\n",
    "print(\"Total of text loaded:\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970f794",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354469b",
   "metadata": {},
   "source": [
    "### Vocabulary extraction\n",
    "Initialize the vocabulary from the traning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0f67f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocalubary size: 48016\n",
      "Bigrams:\n",
      "[(100257, 45767), (45767, 1776), (1776, 9769), (9769, 70), (70, 2194)] ...\n",
      "[('<|endoftext|>', 'Um'), ('Um', ' sl'), (' sl', '么'), ('么', 'g'), ('g', 'ane')] ...\n"
     ]
    }
   ],
   "source": [
    "    # Create a set of bigrams_dict and its frequencies\n",
    "    texts_tokens = []\n",
    "    vocabulary = None\n",
    "    for txt in texts:\n",
    "        cod_tokens = encode(txt)\n",
    "        txt_tokens = decode_single_token(cod_tokens)\n",
    "        if vocabulary:\n",
    "            vocabulary = vocabulary.union(txt_tokens)\n",
    "        else:\n",
    "            vocabulary = set(txt_tokens)\n",
    "        bigrams_dict = compute_bigram_frequency(cod_tokens)   \n",
    "        texts_tokens.append(txt_tokens)\n",
    "\n",
    "    # Show bigram\n",
    "    print(\"Vocalubary size:\", len(vocabulary))\n",
    "    print('Bigrams:')\n",
    "    print(list(bigrams_dict.keys())[:5], '...')  \n",
    "    decoded_bigrams_list = decode_bigrams(list(bigrams_dict.keys()))\n",
    "    print(list(decoded_bigrams_list)[:5], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cae66610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams frenquecies:\n",
      "[((100257, 45767), 118), ((45767, 1776), 2), ((1776, 9769), 2), ((9769, 70), 4), ((70, 2194), 2)] ...\n",
      "[(('<|endoftext|>', 'Um'), 118), (('Um', ' sl'), 2), ((' sl', '么'), 2), (('么', 'g'), 4), (('g', 'ane'), 2)] ...\n",
      "Sorted bigrams frenquecies (descending):\n",
      "[((409, 220), 136352), ((991, 220), 66582), ((220, 1049), 59526), ((13, 362), 57300), ((11, 297), 57276)] ...\n",
      "[((' de', ' '), 136352), ((' em', ' '), 66582), ((' ', '200'), 59526), (('.', ' A'), 57300), ((',', ' o'), 57276)] ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show part of the bigrams       \n",
    "print('Bigrams frenquecies:')  \n",
    "bigram_list = list(bigrams_dict.items())\n",
    "print(bigram_list[:5], '...')   \n",
    "tkn_freq = decode_bigram_freq(bigrams_dict)\n",
    "tkn_freq = list(tkn_freq.items())\n",
    "print(tkn_freq[:5], '...')   \n",
    "\n",
    "# Sorted bigrams by frequency\n",
    "print('Sorted bigrams frenquecies (descending):')  \n",
    "bigram_list = sorted(bigrams_dict.items(), key = lambda value: value[1], reverse=True)\n",
    "print(bigram_list[:5], '...')   \n",
    "tkn_freq = decode_bigram_freq(bigrams_dict)\n",
    "tkn_freq = sorted(tkn_freq.items(), key = lambda value: value[1], reverse=True)\n",
    "print(tkn_freq[:5], '...', '\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8d5a1",
   "metadata": {},
   "source": [
    "Get the two most frequently tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a89fc4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequently token (A): ' de'\n",
      "The second most frequently token (A): ' '\n"
     ]
    }
   ],
   "source": [
    "bigram_tk_A = tkn_freq[0][0][0]\n",
    "bigram_tk_B = tkn_freq[0][0][1]\n",
    "print(f\"The most frequently token (A): '{bigram_tk_A}'\")\n",
    "print(f\"The second most frequently token (A): '{bigram_tk_B}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed008829",
   "metadata": {},
   "source": [
    "Sort the vocabulary and move the special token to the begining of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c31f335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is '<|endoftext|>' into the Vocabulary? True \n",
      "  ([' ', ' !', ' !!', ' !=', ' \"'] ...)\n",
      "Is '<|endoftext|>' into the Vocabulary? True \n",
      "  (['<|endoftext|>', ' ', ' !', ' !!', ' !='] ...)\n"
     ]
    }
   ],
   "source": [
    "sort_voc = sorted(vocabulary)\n",
    "print(f\"Is '{end_token}' into the Vocabulary? {end_token in sort_voc} \\n  ({sort_voc[:5]} ...)\")\n",
    "sort_voc.remove(end_token)\n",
    "sort_voc = [end_token] + sort_voc\n",
    "print(f\"Is '{end_token}' into the Vocabulary? {end_token in sort_voc} \\n  ({sort_voc[:5]} ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a7ed0",
   "metadata": {},
   "source": [
    "### Token mappings\n",
    "Create dictionaries to map each token to an integer (<code>stoi</code>) and an integer to a token (<code>itos</code>). They must have the same size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "992a1307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionary: 'stoi'\n",
      "   [('<|endoftext|>', 0), (' ', 1), (' !', 2), (' !!', 3), (' !=', 4), (' \"', 5), (' \"\"', 6)] ...\n",
      "Dicionary: 'itos'\n",
      "   [(0, '<|endoftext|>'), (1, ' '), (2, ' !'), (3, ' !!'), (4, ' !='), (5, ' \"'), (6, ' \"\"')] ...\n",
      "\n",
      "Vocabulary size: 48016\n",
      "stoi: 48016\n",
      "itos: 48016 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Maps it token (string) to a integer (sequencialy). For simplification we make the 'end_token' be the first element of the dictionaries ('stoi' and 'itos')\n",
    "stoi = {s:i for i, s in enumerate(sort_voc)}  # stoi - string (word) to integer    \n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "print(\"Dicionary: 'stoi'\")\n",
    "print(\"  \", list(stoi.items())[:7], '...')\n",
    "print(\"Dicionary: 'itos'\")\n",
    "print(\"  \", list(itos.items())[:7], '...')\n",
    "print(f\"\\nVocabulary size: {len(sort_voc)}\")\n",
    "print(f\"stoi: {len(stoi)}\")\n",
    "print(f\"itos: {len(itos)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9846d7cc",
   "metadata": {},
   "source": [
    "### Frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcd95ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency table:\n",
      "tensor([[ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [25,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
      "       dtype=torch.int32) ...\n",
      "' de' =  15317\n",
      "' ' =  1\n",
      "N[15317, 1] = 68176 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create table of frequencies for bigrams\n",
    "print(\"Frequency table:\")\n",
    "total_tokens = len(stoi)\n",
    "N = torch.zeros((total_tokens, total_tokens), dtype=torch.int32)\n",
    "for text_tkn in texts_tokens:\n",
    "    for tk1, tk2 in zip(text_tkn, text_tkn[1:]):      \n",
    "      r = stoi[tk1] # row index\n",
    "      c = stoi[tk2] # col index\n",
    "      N[r, c] += 1  \n",
    "\n",
    "print(N[0:15,0:15], \"...\")\n",
    "print(f\"'{bigram_tk_A}' = \", stoi[bigram_tk_A])\n",
    "print(f\"'{bigram_tk_B}' = \", stoi[bigram_tk_B])\n",
    "print(f\"N[{stoi[bigram_tk_A]}, {stoi[bigram_tk_B]}] =\", N[stoi[bigram_tk_A], stoi[bigram_tk_B]].item(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7618d036",
   "metadata": {},
   "source": [
    "Compute the probability table of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc496354",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19444/2924800720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute the table of probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtable_probabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtable_probabilities\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtable_probabilities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9222145024 bytes."
     ]
    }
   ],
   "source": [
    "# Compute the table of probabilities\n",
    "table_probabilities = (N+1).float()\n",
    "table_probabilities /= table_probabilities.sum(1, keepdim=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
